{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "MD6Up0Lj0Lb4",
    "outputId": "4882a47f-0e99-4a7f-e838-6c5baa7cdba3"
   },
   "outputs": [],
   "source": [
    "## For colab runs\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# !mkdir -p data\n",
    "# !cp /content/gdrive/My\\ Drive/uploads/a9a.2 ./data # load this data to the specified dir on your drive first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ORqFnwUjzZOx"
   },
   "source": [
    "# DGM Minimal Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9jcDHlTtzZOy"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from torch import autograd\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cmNNVGbxzZO5"
   },
   "outputs": [],
   "source": [
    "def uniform_decompose(N, m, b=8):\n",
    "    \"\"\"\n",
    "    Decomposes N into m terms: a_i, i=1,m;\n",
    "    so that: max_i a_i - min_i a_i <= 1 applies\n",
    "    --------\n",
    "    b is a number of bits used for an integer\n",
    "    in the output array\n",
    "    \"\"\"\n",
    "    terms = np.empty(m, dtype=f'i{b}')\n",
    "    terms[:] = math.floor(N/m)\n",
    "    terms[:N-terms.sum()] += 1\n",
    "    return terms.tolist() \n",
    "\n",
    "\n",
    "def D(y, x):\n",
    "    \"\"\"\n",
    "    Differential operator\n",
    "    \"\"\"\n",
    "    grad = autograd.grad(\n",
    "        outputs=y, inputs=x,\n",
    "        grad_outputs=torch.ones_like(y),\n",
    "        create_graph=True, allow_unused=True)\n",
    "\n",
    "    if len(grad) == 1:\n",
    "        return grad[0]\n",
    "    return grad\n",
    "\n",
    "\n",
    "def metropolis_weights(A):\n",
    "    \"\"\"\n",
    "    A - Adjacency matrix (i.e., symmetric and with zero diagonal)\n",
    "    \"\"\"\n",
    "    A = A / (1 + torch.max(A.sum(1,keepdims=True),A.sum(0,keepdims=True)))\n",
    "    A.as_strided([len(A)], [len(A)+1]).copy_(1-A.sum(1))\n",
    "    return A\n",
    "\n",
    "def dummy_consensus_variation(F):\n",
    "    num_nodes = F.b.size(0)\n",
    "    G = nx.erdos_renyi_graph(num_nodes, .2)\n",
    "    \n",
    "    S = nx.adjacency_matrix(G).todense()\n",
    "    S = F.b.new_tensor(S)\n",
    "    \n",
    "    W = metropolis_weights(S)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cr-lKFZIzZO9"
   },
   "outputs": [],
   "source": [
    "class Objective:\n",
    "    \"\"\"\n",
    "    Base class for optimization functional\n",
    "    \"\"\"\n",
    "    def __init__(self, A, b, num_nodes):\n",
    "        chunk_sizes = uniform_decompose(A.size(0), num_nodes)\n",
    "        self.A = pad_sequence(A.split(chunk_sizes), batch_first=True)\n",
    "        self.b = pad_sequence(b.split(chunk_sizes), batch_first=True)\n",
    "\n",
    "        \n",
    "class LeastSquares(Objective):\n",
    "    def __call__(self, X):\n",
    "        s = '' if X.ndim < 2 else 'i'\n",
    "        Y = torch.einsum(f'ijk,k{s}->ij', self.A, X) - self.b\n",
    "        return Y.square().sum()\n",
    "    \n",
    "    \n",
    "class LogRegression(Objective):\n",
    "    def __call__(self, X):\n",
    "        s = '' if X.ndim < 2 else 'i'\n",
    "        Y = torch.einsum(f'ij,ijk,k{s}->ij', self.b, self.A, X)\n",
    "        Y = torch.logaddexp(-Y, Y.new([0.])).mean()\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorAccumulator:\n",
    "    \"\"\"\n",
    "    Accumulates tensors and performs efficient\n",
    "    chained matrix multiplication\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_length):\n",
    "        self.n = seq_length\n",
    "        self.acc = []\n",
    "    \n",
    "    def append(self, X):\n",
    "        if len(self.acc) >= self.n:\n",
    "            self.acc.pop(0)\n",
    "        self.acc.append(X)\n",
    "            \n",
    "    def mm(self, X):\n",
    "        return torch.chain_matmul(X, *self.acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sDcmlc5JzZPC"
   },
   "outputs": [],
   "source": [
    "class DGM:\n",
    "    \"\"\"\n",
    "    Base class for decentralized gradient methods\n",
    "    \"\"\"\n",
    "    def __init__(self, F, alpha=1.):\n",
    "        self.F = F\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.n = F.b.size(0)\n",
    "        self.gen = lambda : dummy_consensus_variation(F)\n",
    "        self._initLogs()\n",
    "        \n",
    "    def _initLogs(self):\n",
    "        self.logs = {'i': [], 'fn': [], 'dist2con': []}\n",
    "        \n",
    "    def _dist2consensus(self, X):\n",
    "        h = X.new(self.n).fill_(1.)\n",
    "        Q = torch.norm(X - X/self.n @ h[:,None]*h)\n",
    "        return Q\n",
    "        \n",
    "    def _record(self, X, k):\n",
    "        self.logs['dist2con'].append(self._dist2consensus(X).item())\n",
    "        self.logs['fn'].append(self.F(X.mean(1)).item())\n",
    "        self.logs['i'].append(k)\n",
    "        \n",
    "        \n",
    "class EXTRON(DGM):\n",
    "    \"\"\"\n",
    "    ONe-process EXTRA algorithm\n",
    "    \"\"\"\n",
    "    def _step1(self, X0):\n",
    "        W = self._W = self.gen()\n",
    "        X0.requires_grad_(True)\n",
    "        G0 = D(self.F(X0), X0)\n",
    "        with torch.no_grad():\n",
    "            X1 = X0@W - self.alpha*G0\n",
    "        return G0, X1\n",
    "\n",
    "    def _step2(self, X0, G0, X1):\n",
    "        W = self._W = self.gen()\n",
    "        X1.requires_grad_(True)\n",
    "        G1 = D(self.F(X1), X1)\n",
    "        with torch.no_grad():\n",
    "            X2 = X1 - X0/2 + (X1-X0/2)@W - self.alpha*(G1-G0)\n",
    "        return X1, G1, X2\n",
    "\n",
    "    def run(self, X0, G0=None, X1=None, n_iters=10, lp=1):\n",
    "        if G0 is None or X1 is None:\n",
    "            G0, X1 = self._step1(X0)\n",
    "            self._initLogs()\n",
    "            self._record(X1, 0)\n",
    "\n",
    "        for k in range(1, n_iters):\n",
    "            X0, G0, X1 = self._step2(X0, G0, X1)\n",
    "            if k%lp == 0: self._record(X1, k)\n",
    "\n",
    "        return X0, G0, X1\n",
    "\n",
    "\n",
    "class DIGONing(DGM):\n",
    "    \"\"\"\n",
    "    ONe-process DIGing algorithm\n",
    "    \"\"\"\n",
    "    def run(self, X0, G0=None, Y0=None, n_iters=10, lp=1):\n",
    "        if G0 is None or Y0 is None:\n",
    "            self._initLogs()\n",
    "            X0.requires_grad_(True)\n",
    "            Y0 = D(self.F(X0), X0)\n",
    "            G0 = Y0.clone()\n",
    "            \n",
    "        for k in range(1, n_iters):\n",
    "            W = self._W = self.gen()\n",
    "            with torch.no_grad():\n",
    "                X1 = X0@W - self.alpha*Y0\n",
    "                \n",
    "            X1.requires_grad_(True)\n",
    "            G1 = D(self.F(X1), X1)\n",
    "            with torch.no_grad():\n",
    "                Y1 = Y0@W + G1 - G0\n",
    "                \n",
    "            X0, Y0, G0 = X1, Y1, G1\n",
    "            if k%lp == 0: self._record(X0, k)\n",
    "            \n",
    "        return X0, G0, Y0\n",
    "    \n",
    "    \n",
    "class DAGDON(DGM):\n",
    "    \"\"\"\n",
    "    One-process AGD subroutine \n",
    "    \"\"\"\n",
    "    def __init__(self, F, L, mu, T=20):\n",
    "        self.F = F\n",
    "        self.T = T\n",
    "        self.L = L\n",
    "        self.mu = mu\n",
    "        self.n = F.b.size(0)\n",
    "        self.gen = lambda : dummy_consensus_variation(F)\n",
    "        \n",
    "    def run(self, X0, A0=None, n_iters=10, lp=1):\n",
    "        if A0 is None:\n",
    "            a0 = A0 = 0\n",
    "            U0 = X0.clone()\n",
    "            self._initLogs()\n",
    "            \n",
    "        consensus = TensorAccumulator(self.T)\n",
    "        for k in range(1, n_iters):\n",
    "            W = self._W = self.gen()\n",
    "            consensus.append(W)\n",
    "            \n",
    "            a1 = 1 + A0*self.mu\n",
    "            a1 = (a1 + (a1**2 + 4*self.L*A0*a1)**.5)/(2*self.L)\n",
    "            A1 = A0 + a1\n",
    "            \n",
    "            Y = (a1*U0 + A0*X0)/A1\n",
    "            Y.requires_grad_(True)\n",
    "            G = D(self.F(Y), Y)\n",
    "            with torch.no_grad():\n",
    "                V = self.mu*Y + (1+A0*self.mu)*U0 - a1*G\n",
    "                V /= 1 + A0*self.mu + self.mu\n",
    "                U1 = consensus.mm(V)\n",
    "                X1 = (a1*U1 + A0*X0) / A1\n",
    "                \n",
    "            X0, A0 = X1, A1\n",
    "            if k%lp == 0: self._record(X0, k)\n",
    "                \n",
    "        return X0, A0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tkh45M9rzZPG"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "A, b = load_svmlight_file('data/a9a.2')\n",
    "A = torch.Tensor(A.todense()).to(device)\n",
    "b = torch.Tensor(b).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QVD5IJ0HzZPP"
   },
   "outputs": [],
   "source": [
    "num_nodes = 15\n",
    "F = LogRegression(A, b, num_nodes)\n",
    "X0 = torch.zeros(A.size(1), num_nodes).to(device)\n",
    "opt1 = EXTRON(F)\n",
    "opt2 = DIGONing(F)\n",
    "opt3 = DAGDON(F, L=.5, mu=.0, T=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kkFs0HQvzZPT"
   },
   "outputs": [],
   "source": [
    "n_iters = 1000\n",
    "opt1.run(X0, n_iters=n_iters);\n",
    "opt2.run(X0, n_iters=n_iters);\n",
    "opt3.run(X0, n_iters=n_iters);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D-NO2bdazZPX"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(opt1.logs['fn'], label='EXTRA');\n",
    "plt.plot(opt2.logs['fn'], '--', label='DIGing');\n",
    "plt.plot(opt3.logs['fn'], label='AGD');\n",
    "plt.title('Optimization Functional Value over Iteration Number', size=20)\n",
    "plt.xlabel('# k', size=20)\n",
    "plt.ylabel('f(x)', size=20);\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WUEDQVuczZPd"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(opt1.logs['dist2con'], label='EXTRA');\n",
    "plt.plot(opt2.logs['dist2con'], '--', label='DIGing');\n",
    "plt.plot(opt3.logs['dist2con'], label='AGD');\n",
    "plt.title(r'$||X(I-11^T)||^2$', size=20)\n",
    "plt.xlabel('# k', size=20)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IaH8GF1aBsmc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Initialization Cell",
  "colab": {
   "collapsed_sections": [],
   "name": "experiments.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
